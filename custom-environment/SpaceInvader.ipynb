{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "698837e1-f09b-49d1-9709-35575fe77812",
   "metadata": {},
   "source": [
    "### The SpaceInvader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934c9c4-5244-47b0-a008-521f40cbae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import supersuit as ss\n",
    "import torch\n",
    "from pettingzoo.atari import space_invaders_v2\n",
    "from tqdm import trange\n",
    "\n",
    "from agilerl.algorithms.core.registry import HyperparameterConfig, RLParameter\n",
    "from agilerl.algorithms.maddpg import MADDPG\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from agilerl.utils.algo_utils import obs_channels_to_first\n",
    "from agilerl.utils.utils import create_population, observation_space_channels_to_first\n",
    "from agilerl.vector.pz_async_vec_env import AsyncPettingZooVecEnv\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e82cfb1-a344-4b58-8bb5-4f5042079680",
   "metadata": {},
   "outputs": [],
   "source": [
    "NET_CONFIG = {\n",
    "    \"encoder_config\": {\n",
    "        \"channel_size\": [32, 32],\n",
    "        \"kernel_size\": [3, 3], \n",
    "        \"stride_size\": [2, 2], \n",
    "    },\n",
    "    \"head_config\": {\"hidden_size\": [32, 32]}, \n",
    "}\n",
    "INIT_HP = {\n",
    "    \"POPULATION_SIZE\": 1,\n",
    "    \"ALGO\": \"MADDPG\", \n",
    "    \"CHANNELS_LAST\": True,\n",
    "    \"BATCH_SIZE\": 32,  \n",
    "    \"O_U_NOISE\": True, \n",
    "    \"EXPL_NOISE\": 0.1, \n",
    "    \"MEAN_NOISE\": 0.0,  \n",
    "    \"THETA\": 0.15, \n",
    "    \"DT\": 0.01,  \n",
    "    \"LR_ACTOR\": 0.001, \n",
    "    \"LR_CRITIC\": 0.001, \n",
    "    \"GAMMA\": 0.95, \n",
    "    \"MEMORY_SIZE\": 100000, \n",
    "    \"LEARN_STEP\": 100,  \n",
    "    \"TAU\": 0.01,  \n",
    "}\n",
    "num_envs = 8\n",
    "env = space_invaders_v2.parallel_env()\n",
    "env = ss.frame_skip_v0(env, 4)\n",
    "env = ss.clip_reward_v0(env, lower_bound=-1, upper_bound=1)\n",
    "env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "env = ss.frame_stack_v1(env, 4)\n",
    "env = AsyncPettingZooVecEnv([lambda: env for _ in range(num_envs)])\n",
    "env.reset()\n",
    "\n",
    "observation_spaces = [env.single_observation_space(agent) for agent in env.agents]\n",
    "action_spaces = [env.single_action_space(agent) for agent in env.agents]\n",
    "if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "    observation_spaces = [\n",
    "        observation_space_channels_to_first(obs) for obs in observation_spaces\n",
    "    ]\n",
    "INIT_HP[\"AGENT_IDS\"] = env.agents\n",
    "hp_config = HyperparameterConfig(\n",
    "    lr_actor=RLParameter(min=1e-4, max=1e-2),\n",
    "    lr_critic=RLParameter(min=1e-4, max=1e-2),\n",
    "    batch_size=RLParameter(min=8, max=512, dtype=int),\n",
    "    learn_step=RLParameter(\n",
    "        min=20, max=200, dtype=int, grow_factor=1.5, shrink_factor=0.75\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f99d2bd-04fd-4184-b20c-1f59c1b13229",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent: MADDPG = create_population(\n",
    "    INIT_HP[\"ALGO\"],\n",
    "    observation_spaces,\n",
    "    action_spaces,\n",
    "    NET_CONFIG,\n",
    "    INIT_HP,\n",
    "    hp_config,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    num_envs=num_envs,\n",
    "    device=device,\n",
    ")[0]\n",
    "field_names = [\"obs\", \"action\", \"reward\", \"next_obs\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    INIT_HP[\"MEMORY_SIZE\"],\n",
    "    field_names=field_names,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "    device=device,\n",
    ")\n",
    "agent_ids = deepcopy(env.agents)\n",
    "max_steps = 20000 \n",
    "learning_delay = 500 \n",
    "training_steps = 10000 \n",
    "eval_steps = None \n",
    "eval_loop = 1\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d39efd7-a696-429c-b141-6ad0af3f1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training...\")\n",
    "pbar = trange(max_steps, unit=\"step\")\n",
    "while np.less(agent.steps[-1], max_steps):\n",
    "    obs, info = env.reset()  \n",
    "    scores = np.zeros((num_envs, len(agent_ids)))\n",
    "    completed_episode_scores = []\n",
    "    steps = 0\n",
    "    if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "        obs = {agent_id: obs_channels_to_first(s) for agent_id, s in obs.items()}\n",
    "    for idx_step in range(training_steps // num_envs):\n",
    "        cont_actions, discrete_action = agent.get_action(\n",
    "            obs=obs, training=True, infos=info \n",
    "        )\n",
    "        if agent.discrete_actions:\n",
    "            action = discrete_action\n",
    "        else:\n",
    "            action = cont_actions\n",
    "        action = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "        next_obs, reward, termination, truncation, info = env.step(action)\n",
    "        if not termination:\n",
    "            assert False\n",
    "        scores += np.array(list(reward.values())).transpose()\n",
    "        total_steps += num_envs\n",
    "        steps += num_envs\n",
    "        if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "            next_obs = {\n",
    "                agent_id: obs_channels_to_first(ns)\n",
    "                for agent_id, ns in next_obs.items()\n",
    "            }\n",
    "        memory.save_to_memory(\n",
    "            obs,\n",
    "            cont_actions,\n",
    "            reward,\n",
    "            next_obs,\n",
    "            termination,\n",
    "            is_vectorised=True,\n",
    "        )\n",
    "        if agent.learn_step > num_envs:\n",
    "            learn_step = agent.learn_step // num_envs\n",
    "            if (\n",
    "                idx_step % learn_step == 0\n",
    "                and len(memory) >= agent.batch_size\n",
    "                and memory.counter > learning_delay\n",
    "            ):\n",
    "                experiences = memory.sample(agent.batch_size)\n",
    "                agent.learn(experiences)\n",
    "        elif len(memory) >= agent.batch_size and memory.counter > learning_delay:\n",
    "            for _ in range(num_envs // agent.learn_step):\n",
    "                experiences = memory.sample(agent.batch_size)\n",
    "                agent.learn(experiences)\n",
    "\n",
    "        obs = next_obs\n",
    "        \n",
    "        reset_noise_indices = []\n",
    "        term_array = np.array(list(termination.values())).transpose()\n",
    "        trunc_array = np.array(list(truncation.values())).transpose()\n",
    "        for idx, (d, t) in enumerate(zip(term_array, trunc_array)):\n",
    "            if np.any(d) or np.any(t):\n",
    "                completed_episode_scores.append(scores[idx])\n",
    "                agent.scores.append(scores[idx])\n",
    "                scores[idx] = 0\n",
    "                reset_noise_indices.append(idx)\n",
    "        agent.reset_action_noise(reset_noise_indices)\n",
    "\n",
    "    pbar.update(training_steps)\n",
    "    agent.steps[-1] += steps\n",
    "    fitness = agent.test(\n",
    "        env,\n",
    "        swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "        max_steps=eval_steps,\n",
    "        loop=eval_loop,\n",
    "        sum_scores=False,\n",
    "    )\n",
    "    pop_episode_scores = np.array(completed_episode_scores)\n",
    "    mean_scores = np.mean(pop_episode_scores, axis=0)\n",
    "\n",
    "    print(f\"--- Global steps {total_steps} ---\")\n",
    "    print(f\"Steps {agent.steps[-1]}\")\n",
    "    print(\"Scores:\")\n",
    "    for idx, sub_agent in enumerate(agent_ids):\n",
    "        print(f\"    {sub_agent} score: {mean_scores[idx]}\")\n",
    "    print(\"Fitness\")\n",
    "    for idx, sub_agent in enumerate(agent_ids):\n",
    "        print(f\"    {sub_agent} fitness: {fitness[idx]}\")\n",
    "    print(\"Previous 5 fitness avgs\")\n",
    "    for idx, sub_agent in enumerate(agent_ids):\n",
    "        print(\n",
    "            f\"  {sub_agent} fitness average: {np.mean(agent.fitness[-5:], axis=0)[idx]}\"\n",
    "        )\n",
    "    agent.steps.append(agent.steps[-1])\n",
    "path = \"./models/MADDPG\"\n",
    "filename = \"MADDPG_trained_agent.pt\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "save_path = os.path.join(path, filename)\n",
    "agent.save_checkpoint(save_path)\n",
    "\n",
    "pbar.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b454635c-4cfb-4fb4-b0e2-51f126c5eeb9",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d19a36-776a-4c12-8a6d-3b9e69ef4b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "import numpy as np\n",
    "import supersuit as ss\n",
    "import torch\n",
    "from pettingzoo.atari import space_invaders_v2\n",
    "from PIL import Image, ImageDraw\n",
    "from agilerl.algorithms.maddpg import MADDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852cf10c-6aa7-4b93-b7d6-5c27f239d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _label_with_episode_number(frame, episode_num):\n",
    "    im = Image.fromarray(frame)\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "    if np.mean(frame) < 128:\n",
    "        text_color = (255, 255, 255)\n",
    "    else:\n",
    "        text_color = (0, 0, 0)\n",
    "    drawer.text(\n",
    "        (im.size[0] / 20, im.size[1] / 18), f\"Episode: {episode_num+1}\", fill=text_color\n",
    "    )\n",
    "    return im\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = space_invaders_v2.parallel_env(render_mode=\"rgb_array\")\n",
    "channels_last = True \n",
    "if channels_last:\n",
    "    env = ss.frame_skip_v0(env, 4)\n",
    "    env = ss.clip_reward_v0(env, lower_bound=-1, upper_bound=1)\n",
    "    env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "    env = ss.frame_stack_v1(env, 4)\n",
    "env.reset()\n",
    "try:\n",
    "    state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "    one_hot = True\n",
    "except Exception:\n",
    "    state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "    one_hot = False\n",
    "try:\n",
    "    action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "    discrete_actions = True\n",
    "    max_action = None\n",
    "    min_action = None\n",
    "except Exception:\n",
    "    action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "    discrete_actions = False\n",
    "    max_action = [env.action_space(agent).high for agent in env.agents]\n",
    "    min_action = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "if channels_last:\n",
    "    state_dim = [\n",
    "        (state_dim[2], state_dim[0], state_dim[1]) for state_dim in state_dim\n",
    "    ]\n",
    "\n",
    "n_agents = env.num_agents\n",
    "agent_ids = env.agents\n",
    "\n",
    "path = \"./models/MADDPG/MADDPG_trained_agent.pt\"\n",
    "maddpg = MADDPG.load(path, device)\n",
    "\n",
    "episodes = 10\n",
    "max_steps = 500\n",
    "\n",
    "rewards = []\n",
    "frames = []\n",
    "indi_agent_rewards = {\n",
    "    agent_id: [] for agent_id in agent_ids\n",
    "} \n",
    "\n",
    "for ep in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    agent_reward = {agent_id: 0 for agent_id in agent_ids}\n",
    "    score = 0\n",
    "    for _ in range(max_steps):\n",
    "        if channels_last:\n",
    "            state = {\n",
    "                agent_id: np.moveaxis(np.expand_dims(s, 0), [3], [1])\n",
    "                for agent_id, s in state.items()\n",
    "            }\n",
    "        cont_actions, discrete_action = maddpg.get_action(\n",
    "            state, training=False, infos=info\n",
    "        )\n",
    "        if maddpg.discrete_actions:\n",
    "            action = discrete_action\n",
    "        else:\n",
    "            action = cont_actions\n",
    "        frame = env.render()\n",
    "        frames.append(_label_with_episode_number(frame, episode_num=ep))\n",
    "\n",
    "        state, reward, termination, truncation, info = env.step(\n",
    "            {agent: a.squeeze() for agent, a in action.items()}\n",
    "        )\n",
    "        for agent_id, r in reward.items():\n",
    "            agent_reward[agent_id] += r\n",
    "        score = sum(agent_reward.values())\n",
    "        if any(truncation.values()) or any(termination.values()):\n",
    "            break\n",
    "\n",
    "    rewards.append(score)\n",
    "    for agent_id in agent_ids:\n",
    "        indi_agent_rewards[agent_id].append(agent_reward[agent_id])\n",
    "\n",
    "    print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n",
    "    print(\"Episodic Reward: \", rewards[-1])\n",
    "    for agent_id, reward_list in indi_agent_rewards.items():\n",
    "        print(f\"{agent_id} reward: {reward_list[-1]}\")\n",
    "env.close()\n",
    "gif_path = \"./videos/\"\n",
    "os.makedirs(gif_path, exist_ok=True)\n",
    "imageio.mimwrite(\n",
    "    os.path.join(\"./videos/\", \"space_invaders.gif\"), frames, duration=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad24e20-b04b-4918-9701-f40a2e315c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
